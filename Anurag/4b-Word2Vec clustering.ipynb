{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basing the implementation on the one performed by Pier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19001, 24)\n",
      "(16689, 24)\n",
      "Removed 2312 duplicates (based on RequiredQual)\n",
      "0    To perform this job successfully, an\\nindividu...\n",
      "1    - Bachelor's Degree; Master's is preferred;\\n-...\n",
      "2    - Degree in environmentally related field, or ...\n",
      "3    - Advanced degree in public health, social sci...\n",
      "4    - University degree; economical background is ...\n",
      "Name: RequiredQual, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date                                              Title  \\\n",
      "0   Jan 5, 2004                            Chief Financial Officer   \n",
      "1   Jan 7, 2004  Full-time Community Connections Intern (paid i...   \n",
      "2   Jan 7, 2004                                Country Coordinator   \n",
      "3   Jan 7, 2004                                     BCC Specialist   \n",
      "4  Jan 10, 2004                                 Software Developer   \n",
      "\n",
      "                                           Company AnnouncementCode Term  \\\n",
      "0             AMERIA Investment Consulting Company              NaN  NaN   \n",
      "1  International Research & Exchanges Board (IREX)              NaN  NaN   \n",
      "2        Caucasus Environmental NGO Network (CENN)              NaN  NaN   \n",
      "3                                     Manoff Group              NaN  NaN   \n",
      "4                           Yerevan Brandy Company              NaN  NaN   \n",
      "\n",
      "  Eligibility Audience StartDate                             Duration  \\\n",
      "0         NaN      NaN       NaN                                  NaN   \n",
      "1         NaN      NaN       NaN                             3 months   \n",
      "2         NaN      NaN       NaN  Renewable annual contract\\nPOSITION   \n",
      "3         NaN      NaN       NaN                                  NaN   \n",
      "4         NaN      NaN       NaN                                  NaN   \n",
      "\n",
      "                                            Location  \\\n",
      "0                                   Yerevan, Armenia   \n",
      "1  IREX Armenia Main Office; Yerevan, Armenia \\nD...   \n",
      "2                                   Yerevan, Armenia   \n",
      "3                                Manila, Philippines   \n",
      "4                                   Yerevan, Armenia   \n",
      "\n",
      "                         ...                          \\\n",
      "0                        ...                           \n",
      "1                        ...                           \n",
      "2                        ...                           \n",
      "3                        ...                           \n",
      "4                        ...                           \n",
      "\n",
      "                                        ApplicationP OpeningDate  \\\n",
      "0  To apply for this position, please submit a\\nc...         NaN   \n",
      "1  Please submit a cover letter and resume to:\\nI...         NaN   \n",
      "2  Please send resume or CV toursula.kazarian@......         NaN   \n",
      "3  Please send cover letter and resume to Amy\\nPe...         NaN   \n",
      "4  Successful candidates should submit\\n- CV; \\n-...         NaN   \n",
      "\n",
      "                                      Deadline Notes  \\\n",
      "0                              26 January 2004   NaN   \n",
      "1                              12 January 2004   NaN   \n",
      "2  20 January 2004\\nSTART DATE:  February 2004   NaN   \n",
      "3      23 January 2004\\nSTART DATE:  Immediate   NaN   \n",
      "4                       20 January 2004, 18:00   NaN   \n",
      "\n",
      "                                              AboutC Attach  Year Month  \\\n",
      "0                                                NaN    NaN  2004     1   \n",
      "1  The International Research & Exchanges Board (...    NaN  2004     1   \n",
      "2  The Caucasus Environmental NGO Network is a\\nn...    NaN  2004     1   \n",
      "3                                                NaN    NaN  2004     1   \n",
      "4                                                NaN    NaN  2004     1   \n",
      "\n",
      "      IT                             RequiredQual_processed  \n",
      "0  False  [to, perform, this, job, successfully, an, ind...  \n",
      "1  False  [bachelor, degree, master, is, preferred, exce...  \n",
      "2  False  [degree, in, environmentally, related, field, ...  \n",
      "3  False  [advanced, degree, in, public, health, social,...  \n",
      "4   True  [university, degree, economical, background, i...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import gensim \n",
    "\n",
    "df_ori = pd.read_csv('../data/data job posts.csv')\n",
    "df_ori.head()\n",
    "print(df_ori.shape)\n",
    "df = df_ori.drop_duplicates(['RequiredQual'])\n",
    "print(df.shape)\n",
    "print(\"Removed {0} duplicates (based on RequiredQual)\".format(df_ori.shape[0]-df.shape[0]))\n",
    "\n",
    "print(df[\"RequiredQual\"].head())\n",
    "\n",
    "df[\"RequiredQual\"] = df[\"RequiredQual\"].astype(str)\n",
    "\n",
    "df['RequiredQual_processed'] = df.RequiredQual.apply(gensim.utils.simple_preprocess)\n",
    "df = df.drop([\"RequiredQual\", \"jobpost\"], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the word 2 vec model on the required qualifications after basic pre-processing [refer](http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W7l-lWgzbIU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_model = gensim.models.Word2Vec(df['RequiredQual_processed'], size=100, window=5, min_count=1, workers=4)\n",
    "#word2vec_model.save(\"../outputs/required_qual.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_qual_model = gensim.models.Word2Vec.load(\"../outputs/required_qual.model\")\n",
    "required_qual_model.wv['finance'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the average word 2 vec vectors for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_word2vec_for_qual(token_list):\n",
    "    if token_list:\n",
    "        num_tokens = len(token_list)\n",
    "        word_2_vec_sum = np.zeros(100)\n",
    "        for token in token_list:\n",
    "            word_2_vec_sum += required_qual_model.wv[token]\n",
    "        \n",
    "        #print(\"SUM: \" + str(word_2_vec_sum))\n",
    "        #print(\"LEN: \" + str(num_tokens))\n",
    "        average_word_2_vec = word_2_vec_sum/num_tokens\n",
    "        #print(\"AVG\" + str(average_word_2_vec))\n",
    "        return average_word_2_vec\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10884372, -0.39476334, -0.07866897, -0.03640246, -0.42963778,\n",
       "       -1.05963732, -0.10694249, -0.6946278 , -0.23645025, -0.40056211,\n",
       "        0.34469219,  0.35970644,  0.31005736, -0.26279422, -0.16430086,\n",
       "       -0.04659212,  0.33829459,  0.42903162, -0.07504804,  0.07977514,\n",
       "        0.13282309, -0.18764175,  0.01966968,  0.33593797, -0.57092042,\n",
       "       -0.21030081,  0.10690218, -0.20199957,  0.54535492, -0.8270151 ,\n",
       "       -0.0185453 , -0.11861162, -0.14917046, -0.02694462, -0.21019031,\n",
       "        0.45789709, -0.18619073, -0.44579528, -0.05107523,  0.02861218,\n",
       "       -0.43856222, -0.15693517, -0.019054  ,  0.21764722,  0.12828929,\n",
       "       -0.0537546 , -0.30590729, -0.15455936,  0.04036621, -0.02763412,\n",
       "        0.19034366, -0.38321557, -0.34897171,  0.14813457, -0.33465318,\n",
       "       -0.12326898,  0.46683668,  0.04076201,  0.46264456, -0.31586635,\n",
       "       -0.13638585, -0.02607759,  0.0975218 ,  0.3020699 , -0.09764642,\n",
       "       -0.18479594, -0.73101058,  0.35582876,  0.10177838, -0.17911468,\n",
       "       -0.20146795,  0.53538734,  0.01374152,  0.15188233, -0.33704603,\n",
       "        0.1266277 ,  0.27540627,  0.14264081, -0.02808508, -0.09187064,\n",
       "        0.08289824, -0.32210792, -0.00108986,  0.54090003,  0.16412393,\n",
       "        0.36473437, -0.02979398, -0.22648219, -0.07237939, -0.5441279 ,\n",
       "       -0.17610509, -0.05952008, -0.08513503,  0.3348424 ,  0.38577529,\n",
       "       -0.18693376, -0.18459953, -0.10734701, -0.07698515,  0.20791829])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_word2vec_for_qual(df['RequiredQual_processed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.10884372287936503, -0.3947633358119212, -0....\n",
       "1    [-0.05059807275732358, -0.3633127188723948, 0....\n",
       "2    [-0.06786293610930443, -0.16080295515379736, -...\n",
       "3    [0.031613347048146855, -0.3168140259726594, -0...\n",
       "4    [0.14961214930835096, -0.47536581445654685, -0...\n",
       "Name: RequiredQual_Avg_Word2Vec, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"RequiredQual_Avg_Word2Vec\"] = df['RequiredQual_processed'].apply(get_average_word2vec_for_qual)\n",
    "df[\"RequiredQual_Avg_Word2Vec\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16689"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows = df[\"RequiredQual_Avg_Word2Vec\"].shape[0]\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16689,)\n",
      "(16689, 100)\n"
     ]
    }
   ],
   "source": [
    "word_2_vec_values = df[\"RequiredQual_Avg_Word2Vec\"].values\n",
    "print(word_2_vec_values.shape)\n",
    "word_2_vec_values[0]\n",
    "word_2_vec_values = np.array([np.array(x) for x in word_2_vec_values])\n",
    "print(word_2_vec_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10884372, -0.39476334, -0.07866897, -0.03640246, -0.42963778,\n",
       "       -1.05963732, -0.10694249, -0.6946278 , -0.23645025, -0.40056211,\n",
       "        0.34469219,  0.35970644,  0.31005736, -0.26279422, -0.16430086,\n",
       "       -0.04659212,  0.33829459,  0.42903162, -0.07504804,  0.07977514,\n",
       "        0.13282309, -0.18764175,  0.01966968,  0.33593797, -0.57092042,\n",
       "       -0.21030081,  0.10690218, -0.20199957,  0.54535492, -0.8270151 ,\n",
       "       -0.0185453 , -0.11861162, -0.14917046, -0.02694462, -0.21019031,\n",
       "        0.45789709, -0.18619073, -0.44579528, -0.05107523,  0.02861218,\n",
       "       -0.43856222, -0.15693517, -0.019054  ,  0.21764722,  0.12828929,\n",
       "       -0.0537546 , -0.30590729, -0.15455936,  0.04036621, -0.02763412,\n",
       "        0.19034366, -0.38321557, -0.34897171,  0.14813457, -0.33465318,\n",
       "       -0.12326898,  0.46683668,  0.04076201,  0.46264456, -0.31586635,\n",
       "       -0.13638585, -0.02607759,  0.0975218 ,  0.3020699 , -0.09764642,\n",
       "       -0.18479594, -0.73101058,  0.35582876,  0.10177838, -0.17911468,\n",
       "       -0.20146795,  0.53538734,  0.01374152,  0.15188233, -0.33704603,\n",
       "        0.1266277 ,  0.27540627,  0.14264081, -0.02808508, -0.09187064,\n",
       "        0.08289824, -0.32210792, -0.00108986,  0.54090003,  0.16412393,\n",
       "        0.36473437, -0.02979398, -0.22648219, -0.07237939, -0.5441279 ,\n",
       "       -0.17610509, -0.05952008, -0.08513503,  0.3348424 ,  0.38577529,\n",
       "       -0.18693376, -0.18459953, -0.10734701, -0.07698515,  0.20791829])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_vec_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16689,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++')\n",
    "idx = kmeans_clustering.fit_predict(word_2_vec_values)\n",
    "print(idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 2, 1, 1, 2, 0, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    0\n",
       "3    2\n",
       "4    2\n",
       "Name: RequiredQual_Cluster_Index, dtype: int32"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"RequiredQual_Cluster_Index\"] = idx\n",
    "df[\"RequiredQual_Cluster_Index\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Special processing to remove the unwanted terms\n",
    "def preprocess(tokens):\n",
    "    tokens_nop = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens_nop = [t.lower() for t in tokens_nop]\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    stop = stopwords.words('english')\n",
    "    tokens_nostop = [t for t in tokens_nop if t not in stop]\n",
    "    tokens_lem = [wnl.lemmatize(t) for t in tokens_nostop]\n",
    "    tokens_clean = [t for t in tokens_lem if len(t) >= 3]  # simple way to remove the offending \" punctuations\n",
    "    return tokens_clean\n",
    "\n",
    "\n",
    "df['RequiredQual_processed'] = df[\"RequiredQual_processed\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient for clusters: 0.156\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient for clusters: %0.3f\"\n",
    "          % metrics.silhouette_score(word_2_vec_values, kmeans_clustering.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RequiredQual_Cluster_Index\n",
       "0    [degree, environmentally, related, field, year...\n",
       "1    [bachelor, degree, master, preferred, excellen...\n",
       "2    [perform, job, successfully, individual, must,...\n",
       "Name: RequiredQual_processed, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = df.groupby(\"RequiredQual_Cluster_Index\")[\"RequiredQual_processed\"].sum()\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166343"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"RequiredQual_processed\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster # 0\n",
      "knowledge experience skill language english year work armenian excellent good russian degree least field accounting computer education university communication higher plus office ability relevant finance economics management related banking strong professional fluency software team financial legislation international excel bank written working business preferably law word analytical system technical preferable advanced\n",
      "Cluster # 1\n",
      "skill ability knowledge work experience language excellent english communication armenian good russian strong year degree team computer management least field written office university pressure education personality plus organizational interpersonal higher working analytical high business relevant fluency excel related word responsibility problem time accounting sense detail project self economics oriented oral\n",
      "Cluster # 2\n",
      "experience knowledge ability skill work year language good development english degree least strong excellent communication management team design plus software system field related computer working project web understanding application science technical written sql technology database programming environment server armenian net university tool relevant problem testing preferred html familiarity russian business\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for idx, g in enumerate(grouped_df):\n",
    "    c = Counter(g)\n",
    "    print(\"Cluster # {0}\".format(idx))\n",
    "    tokens = [x[0] for x in c.most_common(50)]\n",
    "    print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from the clusters\n",
    "a. Language skill is required in almost all the job positions - esp. Armenian, Russian and English\n",
    "b. Cluster 0 is more on the finance, 1 (?) and 2 is mostly IT related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
