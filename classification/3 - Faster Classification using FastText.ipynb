{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 - Faster Classification using FastText.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RREn6iHds7GD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "9aedec20-091e-4fcf-c43f-9da10d496e30"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7HdOBdhAwrtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "91ebab4c-0b97-4bf8-fd43-b50997a61c37"
      },
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.26.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.8.24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4lo0TUwy1NuQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "aa13a900-1cce-4c67-9f10-13287684b24c"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.18)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.18 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.18)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.18->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.18->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dxiZ98_vw8ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "e2f66887-217d-4c5e-8af3-706bae7154fc"
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OFM3ndSx2Fhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7E0bZBkz4LaZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download google drive file\n",
        "def download_drive_file(drive_directory, filename):\n",
        "  list_file_query = \"title='{}' and trashed=false\".format(drive_directory)\n",
        "  file_list = drive.ListFile({'q': list_file_query}).GetList()\n",
        "\n",
        "  if len(file_list) > 0:\n",
        "    directory_id = file_list[0]['id']\n",
        "\n",
        "    list_file_query = \"'{}' in parents\".format(directory_id)\n",
        "\n",
        "    file_list = drive.ListFile({'q': list_file_query}).GetList()\n",
        "    \n",
        "    file_id = None\n",
        "    for file1 in file_list:\n",
        "      if file1['title'] == filename:\n",
        "        print(\"downloading file {}\".format(file1['title']))\n",
        "        file1.GetContentFile(file1['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J1JPJ0hn5Nys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5a58ed9-6046-4d43-d9d9-b2e7d7952dd5"
      },
      "cell_type": "code",
      "source": [
        "download_drive_file(\"Datasets\", \"data_jobposts_it.csv\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading file data_jobposts_it.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bra3tLxDrS7H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 - Faster Classification using FastText"
      ]
    },
    {
      "metadata": {
        "id": "Naw-NDherS7I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the previous notebook, we managed to achieve a decent test accuracy of ~85% using all of the common techniques used for sentiment analysis. In this notebook, we'll implement a model that achieves comparable results a lot faster. More specifically, we'll be implementing the \"FastText\" model from the paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759).\n",
        "\n",
        "This will allow us to achieve the same ~85% test accuracy as the last model, but much faster.\n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "One of the key concepts in the FastText paper is that they calculate the n-grams of an input sentence and append them to the end of a sentence. Here, we'll use bi-grams. Briefly, a bi-gram is a pair of words/tokens that appear consecutively within a sentence.\n",
        "\n",
        "For example, in the sentence \"how are you ?\", the bi-grams are: \"how are\", \"are you\" and \"you ?\".\n",
        "\n",
        "The generate_bigrams function takes a sentence that has already been tokenized, calculates the bi-grams and appends them to the end of the tokenized list."
      ]
    },
    {
      "metadata": {
        "id": "nnfcCd7XoOPr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_bigrams(x):\n",
        "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "    for n_gram in n_grams:\n",
        "        x.append(' '.join(n_gram))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQIwqYCboQxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As an example:"
      ]
    },
    {
      "metadata": {
        "id": "lcfaBO5MoQCr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9ccb43ae-1e58-4723-d273-e1e547d395de"
      },
      "cell_type": "code",
      "source": [
        "generate_bigrams(['This', 'job', 'requires', 'you', 'to', 'be', 'fluent', 'in', 'Mandarin'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'job',\n",
              " 'requires',\n",
              " 'you',\n",
              " 'to',\n",
              " 'be',\n",
              " 'fluent',\n",
              " 'in',\n",
              " 'Mandarin',\n",
              " 'This job',\n",
              " 'to be',\n",
              " 'job requires',\n",
              " 'fluent in',\n",
              " 'requires you',\n",
              " 'in Mandarin',\n",
              " 'be fluent',\n",
              " 'you to']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "C_HcDBHDocss",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`TorchText` Fields have a preprocessing argument. A function passed here will be applied to a sentence after it has been tokenized (transformed from a string into a list of tokens), but before it has been indexed (transformed from a token to an integer). Here, we pass our `generate_bigrams` function."
      ]
    },
    {
      "metadata": {
        "id": "XiQUTI1frS7K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy', preprocessing=generate_bigrams)\n",
        "LABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
        "\n",
        "pos = data.TabularDataset(\n",
        "    path='data_jobposts_it.csv', format='csv', \n",
        "    fields=[\n",
        "        ('text', TEXT), \n",
        "        ('label', LABEL)\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_data, test_data = pos.split()\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "26e3b3xlY4N9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build the vocab and load the pre-trained word embeddings."
      ]
    },
    {
      "metadata": {
        "id": "WwpEQ5GyY2d2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=50000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qo9pQwevrS7s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And create the iterators."
      ]
    },
    {
      "metadata": {
        "id": "EFiVT5I9rS7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R96B8UBVrS7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the Model\n",
        "\n",
        "This model has far fewer parameters than the previous model as it only has 2 layers that have any parameters, the embedding layer and the linear layer. There is no RNN component in sight!\n",
        "\n",
        "Instead, it first calculates the word embedding for each word using the Embedding layer, then calculates the average of all of the word embeddings and feeds this through the Linear layer, and that's it!\n",
        "\n",
        "![](https://camo.githubusercontent.com/faa26b515c1fe979636b2e8ca433bacb71815d0d/68747470733a2f2f692e696d6775722e636f6d2f653073575a6f5a2e706e67)\n",
        "\n",
        "We implement the averaging with the `avg_pool2d` (average pool 2-dimensions) function. Initially, you may think using a 2-dimensional pooling seems strange, surely our sentences are 1-dimensional, not 2-dimensional? However, you can think of the word embeddings as a 2-dimensional grid, where the ones are along one axis and the dimensions of the word embeddings are along another. In the image below is an example sentence after being converted into 5-dimensional word embeddings, with the words along the vertical axis and the embeddings along the horizontal axis.\n",
        "\n",
        "![](https://camo.githubusercontent.com/e5d4a50200df0c001003675cf42280a21df1de6d/68747470733a2f2f692e696d6775722e636f6d2f53534832354e542e706e67)\n",
        "\n",
        "The `avg_pool2d` passes a filter of size `embedded.shape[1]` (i.e. the length of the sentence) by 1. This is shown in pink in the image below.\n",
        "\n",
        "![](https://camo.githubusercontent.com/97ed7e6f89deaac7884c664382a3f37b46dcbf3d/68747470733a2f2f692e696d6775722e636f6d2f553765526e49652e706e67)\n",
        "\n",
        "The average value of all of the dimensions is calculated and concatenated into a 5-dimensional (in our pictoral examples, 100-dimensional in the code) tensor for each sentence. This tensor is then passed through the linear layer to produce our prediction."
      ]
    },
    {
      "metadata": {
        "id": "18zc0D2KrS7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FastText(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
        "        \n",
        "        #pooled = [batch size, embedding_dim]\n",
        "                \n",
        "        return self.fc(pooled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bq4FY1eurS7w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As previously, we'll create an instance of our FastText class."
      ]
    },
    {
      "metadata": {
        "id": "uD8-4kS3rS7x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dstSKG2Mcsya",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And copy the pre-trained vectors to our embedding layer."
      ]
    },
    {
      "metadata": {
        "id": "FKnJc3SEcy70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1d39eb95-e721-4137-df2f-ed2718c79365"
      },
      "cell_type": "code",
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "L4xDY8CLrS7y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "\n",
        "Training the model is the exact same as last time.\n",
        "\n",
        "We initialize our optimizer..."
      ]
    },
    {
      "metadata": {
        "id": "etHOvPERrS7z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNHAtqfwrS70",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The rest of the steps for training the model are unchanged.\n",
        "\n",
        "We define the criterion and place the model and criterion on the GPU (if available)..."
      ]
    },
    {
      "metadata": {
        "id": "zyrfpvctrS71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b3UefxVyrS73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We implement the function to calculate accuracy..."
      ]
    },
    {
      "metadata": {
        "id": "h7-a98l_rS73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6vld5loxrS74",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define a function for training our model...\n",
        "\n",
        "**Note:** we are no longer using dropout so we do not need to use `model.train()`, but as mentioned in the 1st notebook, it is good practice to use it."
      ]
    },
    {
      "metadata": {
        "id": "CPdnwRhkrS74",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWaYhAD_rS75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define a function for testing our model...\n",
        "\n",
        "**Note**: again, we leave `model.eval()` even though we do not use dropout."
      ]
    },
    {
      "metadata": {
        "id": "AeLO_teFrS76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00SZAY7DrS76",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we train our model..."
      ]
    },
    {
      "metadata": {
        "id": "LOB6qia4rS77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "cf9b0923-5f68-4fc8-de7f-2a5a1e15a3f9"
      },
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train Loss: 0.541, Train Acc: 80.02%, Val. Loss: 0.463, Val. Acc: 80.39%\n",
            "Epoch: 02, Train Loss: 0.468, Train Acc: 80.00%, Val. Loss: 0.414, Val. Acc: 80.39%\n",
            "Epoch: 03, Train Loss: 0.429, Train Acc: 80.07%, Val. Loss: 0.335, Val. Acc: 83.11%\n",
            "Epoch: 04, Train Loss: 0.373, Train Acc: 81.54%, Val. Loss: 0.257, Val. Acc: 89.27%\n",
            "Epoch: 05, Train Loss: 0.316, Train Acc: 85.83%, Val. Loss: 0.251, Val. Acc: 90.16%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kxd7wzqZrS78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...and get the test accuracy!\n",
        "\n",
        "The results are not as good to the results in the last notebook, but training takes considerably less time."
      ]
    },
    {
      "metadata": {
        "id": "lczIjqZjrS79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "bccQWZhlrS79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, the metric you actually care about, the test loss and accuracy."
      ]
    },
    {
      "metadata": {
        "id": "V4WI7kR7rS79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "02ac0780-e9a2-4495-c843-35f0886af326"
      },
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.224, Test Acc: 90.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CzTv6vfBrS7_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_binary(preds):\n",
        "    \"\"\"\n",
        "    Convert predicted torch array to either 0 or 1\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    return torch.round(F.sigmoid(preds))\n",
        "\n",
        "def predict(model, iterator):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = [] \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = to_binary(model(batch.text).squeeze(1)).cpu().numpy()\n",
        "            all_predictions += predictions.tolist()\n",
        "            \n",
        "    return all_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I4e-9oSGrS8A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the confusion matrix based on predicted and actual test label."
      ]
    },
    {
      "metadata": {
        "id": "PVny0wu6rS8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "4cbbb9e0-b671-4b94-f2ca-821846b735ef"
      },
      "cell_type": "code",
      "source": [
        "test_predicted_labels = predict(model, test_iterator)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "eaa5kESArS8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "54ba1baf-f843-491b-82fd-2150a3e51c21"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "test_actual_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        test_actual_labels += batch.label.cpu().numpy().tolist()\n",
        "        \n",
        "cf = confusion_matrix(test_actual_labels, test_predicted_labels)\n",
        "cf"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4336,  251],\n",
              "       [ 270,  844]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "0a_NEklVrS8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a2b72f7-d37c-43fa-afca-8f249e5d7cb2"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "test_actual_labels_hist = np.histogram(test_actual_labels, bins=[0, 1, 2])\n",
        "test_actual_labels_hist"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4587, 1114]), array([0, 1, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "skoBifOmrS8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db2d66b2-54e0-401f-822e-ed336035808e"
      },
      "cell_type": "code",
      "source": [
        "test_predicted_labels_hist = np.histogram(test_predicted_labels, bins=[0, 1, 2])\n",
        "test_predicted_labels_hist"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4606, 1095]), array([0, 1, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "yHn-qyeJrS8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can finally save our upgraded RNN model for IT job classification task."
      ]
    },
    {
      "metadata": {
        "id": "W5UjjdwMrS8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "1ff69849-f41f-4742-a34f-c3e33b3900b0"
      },
      "cell_type": "code",
      "source": [
        "torch.save(model, '03_fasttext.pth')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type FastText. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJ0Tyqf2jUw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "4dd454f0-1691-45dc-c1a0-82a3b6e22eb1"
      },
      "cell_type": "code",
      "source": [
        "!ls -lAh"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 90M\n",
            "-rw-r--r-- 1 root root  23M Oct  7 03:19 02_lstm.pth\n",
            "-rw-r--r-- 1 root root  20M Oct  7 04:15 03_fasttext.pth\n",
            "-rw-r--r-- 1 root root 2.5K Oct  7 02:26 adc.json\n",
            "drwxr-xr-x 1 root root 4.0K Oct  7 02:26 .config\n",
            "-rw-r--r-- 1 root root  49M Oct  7 03:43 data_jobposts_it.csv\n",
            "drwxr-xr-x 2 root root 4.0K Sep 28 23:32 sample_data\n",
            "drwxr-xr-x 2 root root 4.0K Oct  7 02:41 .vector_cache\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OOj-jIVVivVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('03_fasttext.pth') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azWCX0XrrS8H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "\n",
        "In the final notebook we'll use convolutional neural networks (CNNs) to perform sentiment analysis, and get our best accuracy yet!"
      ]
    }
  ]
}